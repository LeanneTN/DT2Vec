{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT2Vec on Golden-standard dataset\n",
    "- GSD is available at:  http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import timeit\n",
    "import networkx\n",
    "import community\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "import networkx as nx\n",
    "import scipy.sparse as sps\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    auc,\n",
    "    average_precision_score)\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_curve\n",
    "from community import community_louvain\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sknetwork.clustering import Louvain, BiLouvain, modularity, bimodularity\n",
    "from sknetwork.utils import bipartite2undirected, membership_matrix\n",
    "from sknetwork.visualization import svg_graph, svg_digraph, svg_bigraph\n",
    "from sknetwork.hierarchy import LouvainHierarchy, BiLouvainHierarchy\n",
    "from sknetwork.hierarchy import cut_straight, dasgupta_score, tree_sampling_divergence\n",
    "from sknetwork.visualization import svg_graph, svg_digraph, svg_bigraph, svg_dendrogram\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_POS_RATIO = False # If we have an imbalance dataset change it to True \n",
    "MAIN_DIR = '.'\n",
    "os.chdir(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates machine learning metrics\n",
    "    \n",
    "    Args: real lables and predicted labels\n",
    "\n",
    "    Returns: A dictionary containing all the results\n",
    "    \"\"\"\n",
    "    \n",
    "    Precision = precision_score(y, y_pred, average='binary')\n",
    "    Recall = recall_score(y, y_pred, average='binary')\n",
    "    result = {\n",
    "        'accuracy': metrics.accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, average='binary'),\n",
    "        'recall': recall_score(y, y_pred, average='binary'),\n",
    "        'f1_score': f1_score(y, y_pred, average='binary'),\n",
    "        'average_precision_score': average_precision_score(y, y_pred),\n",
    "        #'f2_score':(5 * Precision * Recall) / (4 * Precision + Recall)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result['ROC'] = metrics.roc_auc_score(y, y_pred)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloading_data(): \n",
    "    \"\"\"\n",
    "    This function downloads the golden-standard dataset from \n",
    "    websit 'http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget/'\n",
    "    and save them in 'MAIN_DIR'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_ids = [\n",
    "        'bind_orfhsa_drug_e.txt', 'bind_orfhsa_drug_ic.txt', 'bind_orfhsa_drug_gpcr.txt', 'bind_orfhsa_drug_nr.txt',\n",
    "        'e_simmat_dc.txt', 'ic_simmat_dc.txt', 'gpcr_simmat_dc.txt', 'nr_simmat_dc.txt',\n",
    "        'e_simmat_dg.txt', 'ic_simmat_dg.txt', 'gpcr_simmat_dg.txt', 'nr_simmat_dg.txt',\n",
    "        'e_admat_dgc.txt', 'ic_admat_dgc.txt', 'gpcr_admat_dgc.txt', 'nr_admat_dgc.txt'\n",
    "    ]\n",
    "\n",
    "    for file_id in file_ids:\n",
    "        print(f'Downloading file {file_id}')\n",
    "\n",
    "        data_endpt = f'http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget/{file_id}'\n",
    "        print(data_endpt)\n",
    "        response = requests.get(data_endpt)\n",
    "\n",
    "        with open(file_id, \"wb\") as output_file:\n",
    "            output_file.write(response.content)\n",
    "\n",
    "    # Convert files to csv\n",
    "    for file_id in file_ids:\n",
    "        print(f'Converting file {file_id[:-4]}.csv')\n",
    "\n",
    "        with open(file_id, 'r', encoding='mac_roman') as fin:\n",
    "            cr = csv.reader(fin, delimiter='\\t')\n",
    "            # Replace commas with full stops\n",
    "\n",
    "            filecontents = [line for line in cr]\n",
    "            filecontents_new = list()\n",
    "            for x in filecontents:\n",
    "                line = [line.replace(',', '.x') for line in x]\n",
    "                filecontents_new.append(line)\n",
    "\n",
    "        with open(file_id[:-4] + '.csv', 'w') as fou:\n",
    "            cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "            cw.writerows(filecontents_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_interarction():\n",
    "    \"\"\"\n",
    "    This function integarat different DTI datasets in \n",
    "    golden-standard dataset (e, ic, gpcr, and nr)\n",
    "    \n",
    "    Returns: A DataFrame containing all DTI\n",
    "    \"\"\"\n",
    "    \n",
    "    file_list = ['bind_orfhsa_drug_e', 'bind_orfhsa_drug_ic', 'bind_orfhsa_drug_gpcr','bind_orfhsa_drug_nr']\n",
    "    df = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        df_tmp = pd.read_csv(f'{file}.csv', header= None)\n",
    "        print(f'len of {file} is {df_tmp.shape[0]}')\n",
    "        \n",
    "        df = df.append(df_tmp)\n",
    "        \n",
    "    df = df.rename(columns={0:'to', 1:'from'})\n",
    "    df['to'] = df['to'].map(lambda x: x.lstrip('hsa:'))\n",
    "    df['weight'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_similarity(file_list):\n",
    "    \"\"\"\n",
    "    This function integrates similarity matrix and convertes to a DataFrame\n",
    "    \n",
    "    Args: Drug/target similarity file names\n",
    "\n",
    "    Returns: A DataFram containing of similarities \n",
    "    \"\"\"\n",
    "\n",
    "    df_total = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        df_tmp = pd.read_csv(f'{file}.csv').set_index(['Unnamed: 0'])\n",
    "        print(f'number of drug/gene in {file} file is :{len(set(list(df_tmp) + df_tmp.index))}')\n",
    "\n",
    "        # remove \"hsa\" for genes\n",
    "        df_tmp.columns = pd.Index(map(lambda x: x.lstrip('hsa'), df_tmp.columns))\n",
    "        df_tmp.index = pd.Index(map(lambda x: x.lstrip('hsa'), df_tmp.index))\n",
    "\n",
    "        df_tmp.values[[np.arange(len(df_tmp))] * 2] = np.nan\n",
    "        df_tmp = df_tmp.stack().reset_index().rename(columns={'level_0': 'to', 'level_1': 'from', 0: 'weight'})\n",
    "        df_tmp = df_tmp[df_tmp['weight'] != 0]\n",
    "\n",
    "        df_total = df_total.append(df_tmp)\n",
    "\n",
    "    num_total = len(set(list(df_total['from']) + list(df_total['to'])))           \n",
    "    print(f'\\nTotal number of unique gene/drug: {num_total}')\n",
    "\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeing_sim_matrix (df, input_data= 'drug'):\n",
    "    \"\"\"\n",
    "    This function merges drugs/targets similarity as an adjacency matrix\n",
    "    \n",
    "    Args: \n",
    "        df: A DataFram containing of similarities\n",
    "        input_data: The type of data ('drug' or 'target')\n",
    "    \n",
    "    Returns: Drugs/targets adjacency matrix\n",
    "    \"\"\"\n",
    "    nodes = df.iloc[:, 0].tolist() + df.iloc[:, 1].tolist()\n",
    "    nodes = sorted(list(set(nodes)))\n",
    "    node_dic = {k: v for v, k in enumerate(sorted(nodes))}\n",
    "    node_dic_rev = dict((y, x) for x, y in node_dic.items())\n",
    "    nodes =[(v, k) for k, v in node_dic.items()]\n",
    "    df_ID = df.copy()\n",
    "    for i in range(len(nodes)):\n",
    "        df_ID = df_ID.replace(nodes[i][1], nodes[i][0])\n",
    "\n",
    "    A = np.array(df_ID.values.tolist())\n",
    "    i, j, weight = A[:, 0], A[:, 1], A[:, 2]\n",
    "    # find the dimension of the square matrix\n",
    "    dim = max(len(set(i)), len(set(j)))\n",
    "\n",
    "    B = sps.lil_matrix((dim, dim))\n",
    "    for i, j, w in zip(i, j, weight):\n",
    "        B[i, j] = w\n",
    "\n",
    "    B = B.todense()\n",
    "    B_temp1 = B + B.T\n",
    "    \n",
    "    n = B_temp1.shape[0]\n",
    "    di = np.diag_indices(n)\n",
    "    B_temp1[di] = 1\n",
    "        \n",
    "    B_new = pd.DataFrame(B_temp1)\n",
    "    B_new = B_new.rename(index=node_dic_rev).rename(columns=node_dic_rev)\n",
    "    \n",
    "    if input_data == 'target':\n",
    "        B_new['from'] = 'hsa' + B_new['from'].astype(str)\n",
    "        B_new['to'] = 'hsa' + B_new['to'].astype(str)\n",
    "        \n",
    "        # It will used in comparing different methods as an input data\n",
    "        B_new.to_csv('all_PPI.txt', sep='\\t')\n",
    "    else:\n",
    "        B_new.to_csv('all_DDI.txt', sep='\\t')\n",
    "        \n",
    "    return B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(X, model, y):\n",
    "    \"\"\"\n",
    "    This function plots the ROC curve and shows the best threshold \n",
    "    \n",
    "    Args: \n",
    "        X: A DataFram containing features\n",
    "        model: Trained machine learning model\n",
    "        y: A list of the labels\n",
    "    \n",
    "    Returns: The value of best threshold\n",
    "    \"\"\"\n",
    "    # predict probabilities\n",
    "    pred_test_proba = model.predict_proba(X)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    pred_test_proba = pred_test_proba[:, 1]\n",
    "    # calculate roc curves\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred_test_proba)\n",
    "    # calculate the g-mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    # locate the index of the largest g-mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print('\\n Best threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "    best_threshold = thresholds[ix]\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "    pyplot.plot(fpr, tpr, marker='.', label='XGBoost')\n",
    "    pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best threshold')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.legend()\n",
    "\n",
    "    return pyplot, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix():\n",
    "    \"\"\"\n",
    "    This function creates adjacency matrix for DTI(for finding negative interactions)\n",
    "    \n",
    "    Returns: DTI adjacency matrix\n",
    "    \"\"\"\n",
    "    DTI_matri_file = ['e_admat_dgc','ic_admat_dgc', 'gpcr_admat_dgc','nr_admat_dgc'] \n",
    "    all_dict = {}\n",
    "    for i in DTI_matri_file:\n",
    "        base_file = i.split('.')[0]\n",
    "        all_dict[base_file] = pd.read_csv('./'+ f'{base_file}.csv', index_col=0)    \n",
    "\n",
    "    df_temp1 = pd.concat([all_dict['ic_admat_dgc'], all_dict['e_admat_dgc']], sort=False)\n",
    "    df_temp2 = pd.concat([df_temp1, all_dict['gpcr_admat_dgc']], sort=False)\n",
    "    all_interaction_matri = pd.concat([df_temp2, all_dict['nr_admat_dgc']], sort=False).fillna(0)\n",
    "    all_interaction_matri.to_csv('golden_standard_dataset/all_interaction.csv')\n",
    "\n",
    "    return all_interaction_matri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    \"\"\"\n",
    "    This function separates features (as x) and labels (as y)\n",
    "    \n",
    "    Args: A Datafram containing features and labels\n",
    "        \n",
    "    Returns: \n",
    "        x: Features\n",
    "        y: labels\n",
    "    \"\"\"\n",
    "    df['label'] = df['weight']\n",
    "    df.drop(columns = ['weight'], inplace=True)\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    X = df.drop(columns=['label'])\n",
    "    y = pd.DataFrame(df['label'])\n",
    "    \n",
    "    return X, y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weight(y_train):\n",
    "    \"\"\"\n",
    "    This function calculates the weigh_ratio for training imbalance dataset\n",
    "    \n",
    "    Args: Labels of train-set\n",
    "    \n",
    "    Returns: The weight ratio\n",
    "    \"\"\"\n",
    "    weight_ratio = float(len(y_train[y_train == 0]))/float(len(y_train[y_train == 1]))\n",
    "    \n",
    "    return weight_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concating(df, embedding):\n",
    "    \"\"\"\n",
    "    This function concats drugs and targets vectors\n",
    "    \n",
    "    Args: \n",
    "        df: A DataFrame of drug-target interactions\n",
    "        embedding:A DataFrame of embedded vectors for each drug and target\n",
    "        \n",
    "    Returns: A DataFrame of drug-target interaction vectors\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for idx, row in df.iterrows():\n",
    "        from_vector = embedding.loc[row['from']]\n",
    "        to_vector = embedding.loc[row['to']]\n",
    "        features = from_vector.append(to_vector).reset_index(drop=True)\n",
    "        features = features.append(row)\n",
    "        dataset.append(features)\n",
    "\n",
    "    df_final = pd.DataFrame(dataset)\n",
    "    df_final.drop(columns=['from', 'to'], inplace=True)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it when you running DT2Vec for the first time\n",
    "downloading_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Reading DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_interaction = reading_interarction()\n",
    "print(f'\\n number of interactions: {df_interaction.shape[0]}')\n",
    "print(df_interaction['weight'].value_counts())\n",
    "df_interaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numGene_DTI = len(set(df_interaction['to']))\n",
    "print(f'number of unige gene in DTI: {numGene_DTI}')\n",
    "numDrug_DTI = len(set(df_interaction['from']))\n",
    "print(f'number of unige drug in DTI: {numDrug_DTI}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Reading PPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPI_file = [\"e_simmat_dg\",\"ic_simmat_dg\",\"gpcr_simmat_dg\",\"nr_simmat_dg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_protein_type = {}\n",
    "# add type of proteins\n",
    "for file in PPI_file:\n",
    "    df_tmp = pd.read_csv(f'/{file}.csv').set_index(['Unnamed: 0'])\n",
    "    df_tmp['type']= file\n",
    "    dic_tmp = df_tmp[['type']].to_dict()['type']\n",
    "    dic_protein_type.update(dic_tmp)\n",
    "\n",
    "dic_protein_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PPI = reading_similarity(PPI_file)\n",
    "df_PPI_all = mergeing_sim_matrix(df_PPI)\n",
    "df_PPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3- Reading DDS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDI_file = [\"e_simmat_dc\",\"ic_simmat_dc\",\"gpcr_simmat_dc\",\"nr_simmat_dc\"]\n",
    "df_DDI = reading_similarity(DDI_file)\n",
    "df_DDI_all = mergeing_sim_matrix(df_DDI)\n",
    "df_DDI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Creating DTI adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_interaction_matri = adjacency_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Changing Ids from str to int for Nod2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all simalrities and interactions as one list (a format needed for N2V)\n",
    "df_all_sim_inter = (df_DDI.append(df_PPI)).append(df_interaction)\n",
    "df_all_sim_inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique gene and drug 989, 791 repectively\n",
    "list_of_str = set(df_all_sim_inter['from'].append(df_all_sim_inter['to']))\n",
    "len(list_of_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creat dic of drug and genes to number (in N2V the name of node should save as number)\n",
    "DrugID2NumericID = {k: v+1 for v, k in enumerate(sorted(list_of_str))}\n",
    "NumericID2DrugID = dict((y,x) for x,y in DrugID2NumericID.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all positive and negative interactions in matrix\n",
    "all_interaction_matri.index = pd.Index(map(lambda x : x.lstrip('hsa'), all_interaction_matri.index))\n",
    "# mapping genes to number\n",
    "all_interaction_matri_newLabel = all_interaction_matri.rename(index=DrugID2NumericID) \n",
    "# mapping drugs to number\n",
    "all_interaction_matri_newLabel = all_interaction_matri_newLabel.rename(columns=DrugID2NumericID) \n",
    "all_interaction_matri_newLabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDS (n = 791  --> n*n-1/2 -->791*790/2= 3312445 )\n",
    "df_DDI_newID = df_DDI.replace(DrugID2NumericID)\n",
    "print(f'\\n all DDS: {df_DDI_newID.shape[0]}')\n",
    "df_DDI_newID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeList_dds = df_DDI_newID.values.tolist()\n",
    "G = networkx.Graph()\n",
    "\n",
    "for i in range(len(edgeList_dds)):\n",
    "    G.add_edge(edgeList_dds[i][0], edgeList_dds[i][1], weight=edgeList_dds[i][2])\n",
    "    \n",
    "A = networkx.adjacency_matrix(G)\n",
    "DDS_adj = A.copy()\n",
    "\n",
    "louvain = Louvain()\n",
    "labels = louvain.fit_transform(DDS_adj)\n",
    "\n",
    "DDS_cluster_label= pd.DataFrame({'drug':list(G.nodes()), 'label':labels})\n",
    "DDS_cluster_label['label']= DDS_cluster_label['label'].astype(str)\n",
    "labels_unique, counts = np.unique(labels, return_counts=True)\n",
    "print(labels_unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = nx.get_edge_attributes(G,'weight').values()\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_hierarchy = LouvainHierarchy()\n",
    "dendrogram = louvain_hierarchy.fit_transform(A)\n",
    "dasgupta_score(A, dendrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_sampling_divergence(A, dendrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug2cluster = dict(zip(DDS_cluster_label.drug, DDS_cluster_label.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict_drug = {k: v for k, v in enumerate( ['mediumvioletred','mediumblue','gold','green','violet','mediumturquoise','mediumvioletred','darkgoldenrod','pink','dimgray'])}\n",
    "color_dict_drug= {str(key): value for key, value in color_dict_drug.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity2 = community.modularity(drug2cluster, G, weight='weight')\n",
    "print(\"The modularity Q based on networkx is {}\".format(modularity2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G, scale=2)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "nx.draw(G, pos, node_color = [color_dict_drug[v] for v in drug2cluster.values()], edge_color=weights, node_size=[30]*len(G.nodes()))\n",
    "plt.savefig('drug_graph_GSD.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPS (n = 989 --> 989*988/2 = 488566)\n",
    "df_PPI_newID = df_PPI.replace(DrugID2NumericID)\n",
    "print(f'all PPS: {df_PPI_newID.shape[0]}')\n",
    "df_PPI_newID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters PPS network\n",
    "edgeList_pps= df_PPI_newID.values.tolist()\n",
    "G = networkx.Graph()\n",
    "\n",
    "for i in range(len(edgeList_pps)):\n",
    "    G.add_edge(edgeList_pps[i][0], edgeList_pps[i][1], weight=edgeList_pps[i][2])\n",
    "    \n",
    "A = networkx.adjacency_matrix(G).A\n",
    "PPS_adj = A.copy()\n",
    "\n",
    "louvain = Louvain()\n",
    "labels = louvain.fit_transform(PPS_adj)\n",
    "\n",
    "labels_unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "PPS_cluster_label= pd.DataFrame({'target':list(G.nodes()), 'label':labels})\n",
    "PPS_cluster_label['label']= PPS_cluster_label['label'].astype(str)\n",
    "print(labels_unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = nx.get_edge_attributes(G,'weight').values()\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_hierarchy = LouvainHierarchy()\n",
    "dendrogram = louvain_hierarchy.fit_transform(A)\n",
    "dasgupta_score(A, dendrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2cluster = dict(zip(PPS_cluster_label.target, PPS_cluster_label.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity2 = community.modularity(target2cluster, G, weight='weight')\n",
    "print(\"The modularity Q based on networkx is {}\".format(modularity2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(list(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [element * 1000 for element in weights]\n",
    "weight = [500 if i>=500 else i for i in weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {k: v for k, v in enumerate(['#58ACFA','#FF1493', 'yellow','orange', '#00CED1','#5F9EA0','#006400','#96bf65','#fcc808','#7b2b48',\n",
    " '#e96957','#e06000','#173679','#d2dd49','#684a6b','#096eb2','#ce482a', 'red', 'lime', 'lightslategray',\n",
    "                                      'olive', 'rosybrown', 'sienna', 'darkmagenta','midnightblue','maroon',\n",
    "                                      'lightcoral','gold','sandybrown','tomato','lawngreen','lightgreen','darkorchid',\n",
    "                                      'lightskyblue','darkgreen'])}\n",
    "color_dict= {str(key): value for key, value in color_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition = community_louvain.best_partition(G)\n",
    "pos = nx.spring_layout(G, k=0.15)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "nx.draw(G, pos, node_color=[color_dict[v] for v in target2cluster.values()], edge_color=weight, node_size=[20]*len(G.nodes()))\n",
    "plt.savefig('target_graph_GSD.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTI\n",
    "df_interaction_newID = df_interaction.replace(DrugID2NumericID)\n",
    "print(f'all DTI: {df_interaction_newID.shape[0]}')\n",
    "df_interaction_newID.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Developing ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(DDS, PPS_seq, mapping=True):\n",
    "    \"\"\"\n",
    "    This function maps drug-drug similarities and protein-protein similarities to vectors\n",
    "    \n",
    "    Args: \n",
    "    PPS_seq: A DataFrame of protein-protein smilarities \n",
    "    DDS: A DataFrame of drug-drug smilarities \n",
    "    mapping: Binary (False/True), read from the saved files (mapped before)\n",
    "    \n",
    "    Returns: A dataframe vectors of drugs and proteins \n",
    "    \"\"\"\n",
    "    \n",
    "    PPS_seq = PPS_seq[PPS_seq['weight']!=0]\n",
    "    DDS = DDS[DDS['weight']!=0]\n",
    "    \n",
    "    DDS.to_csv('edglist_drugs_goldenData.edgelist', sep=' ', index=False, header=False)\n",
    "    PPS_seq.to_csv('edglist_proteins_goldenData.edgelist', sep=' ', index=False, header=False)\n",
    "    \n",
    "    if mapping:\n",
    "        # nod2vec (for drug)\n",
    "        os.system(f'PYTHONHASHSEED=10 python2 node2vec/src/main.py --workers 8 --input edglist_drugs_goldenData.edgelist --output dim100_drugs_goldenData.emb --weighted --dimensions 100')\n",
    "         # nod2vec (for proteins)\n",
    "        os.system(f'PYTHONHASHSEED=10 python2 node2vec/src/main.py --workers 8 --input edglist_proteins_goldenData.edgelist --output dim100_proteins_goldenData.emb --weighted --dimensions 100')\n",
    "\n",
    "    embeddings_seq_drug = pd.read_csv('dim100_drugs_goldenData.emb', sep=' ', skiprows=[0], header=None, index_col=0)  \n",
    "    embeddings_seq_protein = pd.read_csv('/dim100_proteins_goldenData.emb', sep=' ', skiprows=[0], header=None, index_col=0)  \n",
    "\n",
    "    embeddings_seq = embeddings_seq_drug.append(embeddings_seq_protein)\n",
    "    embeddings_seq.index.name = 'ID'\n",
    "    \n",
    "    return embeddings_seq, embeddings_seq_drug, embeddings_seq_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_seq, embeddings_drug, embeddings_protein = embedding(df_PPI_newID, df_DDI_newID, mapping=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_2d_with_clusters(df_total, DDS, PPS_seq, pca_t):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function plot PCA of drugs and proteins vectors \n",
    "    \n",
    "    Args: \n",
    "    PPS_seq: A DataFrame of protein-protein smilarities \n",
    "    DDS: A DataFrame of drug-drug smilarities \n",
    "    df_total: A dataframe vectors of drugs and proteins \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    drug_name = list(set(list(DDS['from'])+list(DDS['to'])))\n",
    "    protein_name = list(set(list(PPS_seq['from'])+list(PPS_seq['to'])))\n",
    "\n",
    "    drug_vec = df_total.loc[drug_name]\n",
    "    drug_vec['type'] = 'Drug'\n",
    "            \n",
    "    protein_vec = df_total.loc[protein_name]   \n",
    "    \n",
    "    if pca_t=='Drug':\n",
    "        df = drug_vec.copy()\n",
    "    elif pca_t== 'Target':\n",
    "        protein_vec['type'] = protein_vec.index\n",
    "        protein_vec['type'] = protein_vec['type'].replace(NumericID2DrugID)\n",
    "        protein_vec['type'] = 'hsa' + protein_vec['type'].astype(str)\n",
    "        protein_vec['type'] = protein_vec['type'].astype(str).replace(dic_protein_type).replace({\"e_simmat_dg\":'Enzymes',\n",
    "                                                                                                 \"ic_simmat_dg\":'ion channels',\n",
    "                                                                                                 \"gpcr_simmat_dg\":'G-protein-coupled receptors',\n",
    "                                                                                                 \"nr_simmat_dg\": 'Nuclear receptors'})\n",
    "        df = protein_vec.copy()\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    if pca_t=='Drug':\n",
    "        pca = TSNE(n_components=2, random_state=42, perplexity=50, n_iter= 400 )\n",
    "    pca_result = pca.fit_transform(df.drop(columns=['type']).values)\n",
    "  \n",
    "    \n",
    "    df['TSNE-1'] = pca_result[:,0]\n",
    "    df['TSNE-2'] = pca_result[:,1]\n",
    "        \n",
    "    chart = ggplot(df, aes(x='TSNE-1', y='TSNE-2',  color='factor(type)') ) \\\n",
    "        + geom_point(size=120, alpha=0.8) \\\n",
    "        + scale_color_manual(values = ['#58ACFA','#FF1493', '#00BFFF','#00CED1','#5F9EA0','#006400','#006400','#96bf65','#fcc808','#7b2b48',\n",
    " '#e96957','#e06000','#173679','#e8a1a2','#d2dd49','#684a6b','#096eb2','#bde1e9','#d2dd49','#ce482a'])\\\n",
    "        #+ geom_text(aes(label='factor(name)'), size=6, color='black')\n",
    "    \n",
    "    \n",
    "    chart.save(f'./{pca_t}_pca_GSD.png', width=12, height=8)  \n",
    "   \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_2d_with_Louvain_clusters(embeddings_seq, drug2cluster, target2cluster, type_data= 'drug'):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function plot PCA of drugs and proteins vectors \n",
    "    \n",
    "    Args: \n",
    "    PPS_seq: A DataFrame of protein-protein smilarities \n",
    "    DDS: A DataFrame of drug-drug smilarities \n",
    "    df_total: A dataframe vectors of drugs and proteins \n",
    "    \n",
    "    \"\"\"\n",
    "    df = embeddings_seq.copy()\n",
    "    df['cluster'] = df.index\n",
    "   # df['cluster'] = df['cluster'].astype(str).replace(NumericID2DrugID)\n",
    "    \n",
    "    if type_data== 'drug':\n",
    "        df = df[df['cluster'].isin(list(drug2cluster.keys()))]\n",
    "        df['cluster'] = df['cluster'].replace(drug2cluster)\n",
    "    else:\n",
    "        df = df[df['cluster'].isin(list(target2cluster.keys()))]\n",
    "        df['cluster'] = df['cluster'].replace(target2cluster)\n",
    "\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    if type_data== 'drug':\n",
    "        pca = TSNE(n_components=2, random_state=42, perplexity=50, n_iter= 400 )\n",
    "    pca_result = pca.fit_transform(df.drop(columns=['cluster']).values)\n",
    "  \n",
    "    if type_data== 'target':\n",
    "        color_ids = ['#58ACFA','#FF1493', 'yellow','orange', '#00CED1','#5F9EA0','#006400','#96bf65','#fcc808','#7b2b48',\n",
    " '#e96957','#e06000','#173679','#d2dd49','#684a6b','#096eb2','#ce482a', 'red', 'lime', 'lightslategray',\n",
    "                                      'olive', 'rosybrown', 'sienna', 'darkmagenta','midnightblue','maroon',\n",
    "                                      'lightcoral','gold','sandybrown','tomato','lawngreen','lightgreen','darkorchid',\n",
    "                                      'lightskyblue','darkgreen']\n",
    "    if type_data== 'drug':\n",
    "        color_ids = ['mediumvioletred','mediumblue','gold','green','violet','mediumturquoise','mediumvioletred','darkgoldenrod','pink','dimgray']\n",
    "\n",
    "    \n",
    "    \n",
    "    df['PCA-1'] = pca_result[:,0]\n",
    "    df['PCA-2'] = pca_result[:,1]\n",
    "        \n",
    "    chart = ggplot(df, aes(x='PCA-1', y='PCA-2',  color='factor(cluster)') ) \\\n",
    "        + geom_point(size=120, alpha=0.8) \\\n",
    "        + scale_color_manual(values = color_ids)    \n",
    "    \n",
    "    chart.save(f'./{type_data}_GSD_pca_cluster.png', width=12, height=8)  \n",
    "        \n",
    "   \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_seq, embeddings_seq_drug, embeddings_seq_protein = embedding(df_PPI_newID, df_DDI_newID, mapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_2d_with_Louvain_clusters(embeddings_seq, drug2cluster, target2cluster, type_data= 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_2d_with_clusters(embeddings_seq, df_DDI_newID, df_PPI_newID, pca_t='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_2d_with_Louvain_clusters(embeddings_seq, drug2cluster, target2cluster, type_data= 'drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_2d_with_clusters(embeddings_seq, df_DDI_newID, df_PPI_newID, pca_t='Drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_seq, embeddings_seq_drug, embeddings_seq_protein = embedding(df_DDI_newID, df_PPI_newID, mapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(df_interaction_newID, all_interaction_matri_newLabel, embeddings , n=10):\n",
    "    \"\"\"\n",
    "    This function ceates the machine learning model (train and test the model)\n",
    "    \n",
    "    Args: \n",
    "        df_interaction_newID: A DataFrame of drug-target interactions\n",
    "        df_PPI_newID: A DataFrame of protein-protein smilarities \n",
    "        df_DDI_newID: A DataFrame of drug-drug smilarities \n",
    "        all_interaction_matri_newLabel: DataFrame of all positive and negative interactions in matrix\n",
    "        n: Number of folds\n",
    "        \n",
    "    Returns: \n",
    "        all_train_results: Traning results for n folds\n",
    "        all_test_results: Testing results for n folds\n",
    "    \"\"\"\n",
    "    ## spilit positive DTIs to external and CV\n",
    "    DTI_external_test = df_interaction_newID.sample(frac = 0.1)\n",
    "    DTI_new = df_interaction_newID.drop(DTI_external_test.index)\n",
    "\n",
    "    num_crossVal = 0 \n",
    "    all_train_results, all_test_results = [], []\n",
    "    kf = KFold(n_splits=n, random_state=42, shuffle=True)\n",
    "    kf.get_n_splits(DTI_new)\n",
    "    for train_index, test_index in kf.split(DTI_new):\n",
    "        df_interaction_newID_train, df_interaction_newID_test = DTI_new.iloc[train_index], DTI_new.iloc[test_index]\n",
    "        num_crossVal = num_crossVal + 1 \n",
    "        print(f'\\n\\n\\n  KFold: {num_crossVal}')\n",
    "        print(f'# all DTI: {DTI_new.shape[0]}')\n",
    "        print(f'# DTI train-set: {df_interaction_newID_train.shape[0]}')\n",
    "        print(f'# DTI test-set: {df_interaction_newID_test.shape[0]}')\n",
    "\n",
    "        # Embeddings: embedding file for all drugs and genes based on DTI, DDS, PPS\n",
    "        # read from saved file\n",
    "\n",
    "        all_interaction_edgeList = all_interaction_matri_newLabel.stack().reset_index().rename(columns={'level_0':'to','level_1':'from',0:'weight'})\n",
    "        all_negative_edgeList = all_interaction_edgeList[all_interaction_edgeList['weight']==0]\n",
    "\n",
    "        ## spilit negative DTIs to external and CV\n",
    "        DTI_external_negative = all_negative_edgeList.sample(frac = 0.1)\n",
    "        DTI_new_negative = all_negative_edgeList.drop(DTI_external_negative.index)\n",
    "\n",
    "        # selecting n negative sample where n is number of train and test\n",
    "        negative_edgeList_sampel = DTI_new_negative.sample(n = DTI_new.shape[0])\n",
    "\n",
    "        # splite them to train and test \n",
    "        p_n = (100 - n)/100\n",
    "        msk = np.random.rand(len(negative_edgeList_sampel)) < p_n\n",
    "        df_negative_train = negative_edgeList_sampel[msk]\n",
    "        df_negative_test = negative_edgeList_sampel[~msk]\n",
    "\n",
    "        print(f'# negative interaction: {negative_edgeList_sampel.shape[0]}')\n",
    "        print(f'# negative train: {df_negative_train.shape[0]}')\n",
    "        print(f'# negative test: {df_negative_test.shape[0]}')\n",
    "\n",
    "        df_train_edj = df_negative_train.append(df_interaction_newID_train)\n",
    "        df_test_edj = df_negative_test.append(df_interaction_newID_test)\n",
    "        df_train = concating(df_train_edj, embeddings)\n",
    "        df_test = concating(df_test_edj, embeddings)\n",
    "\n",
    "        print('=============================================================')\n",
    "        print(f'# train: {df_train.shape[0]}')\n",
    "        print(f'# test: {df_test.shape[0]}')\n",
    "        print('=============================================================')\n",
    "        print(f'training...')\n",
    "\n",
    "        X, y = get_data(df_train.rename(columns={\"label\": \"weight\"}))\n",
    "        print(f'X_train.shape: {X.shape}')\n",
    "\n",
    "        if SCALE_POS_RATIO == True:\n",
    "            weight_ratio = get_sample_weight(y)\n",
    "            model = XGBClassifier(scale_pos_weight=weight_ratio)\n",
    "        else:\n",
    "            model = XGBClassifier(learning_rate= 0.4, max_depth= 4, min_child_weight=2) #defult gamma= 0, min_child_weight=1\n",
    "        model.fit(X, y)\n",
    "\n",
    "        pred_train = model.predict(X)\n",
    "        train_results = calculate_metrics(y, pred_train)\n",
    "        train_results['Kfold'] = num_crossVal\n",
    "        train_results['model'] = model\n",
    "        all_train_results.append(train_results)\n",
    "\n",
    "        print('\\n')\n",
    "        print(f'testing...')\n",
    "\n",
    "        X, y = get_data(df_test.rename(columns={\"label\": \"weight\"}))\n",
    "        print(f'X_test.shape: {X.shape}')\n",
    "\n",
    "        pred_test = model.predict(X)\n",
    "        test_results = calculate_metrics(y, pred_test)\n",
    "        test_results['model'] = model\n",
    "        all_test_results.append(test_results)\n",
    "        best_model = pd.DataFrame(all_test_results).sort_values(by=['average_precision_score'], ascending=False)['model'][0]\n",
    "\n",
    "        del X\n",
    "        del y\n",
    "\n",
    "        ##########################################\n",
    "    \n",
    "        # external testset\n",
    "        DTI_new_negative_sample = DTI_external_negative.sample(n = DTI_external_test.shape[0])\n",
    "        df_all_external = DTI_external_test.append(DTI_new_negative_sample)\n",
    "\n",
    "\n",
    "        df_all_external = concating(df_all_external, embeddings)\n",
    "        X_test_ex, y_test_ex = get_data(df_all_external.rename(columns={\"label\": \"weight\"}))\n",
    "        pred_test_ex = best_model.predict(X_test_ex)\n",
    "        test_results_ex = calculate_metrics(y_test_ex, pred_test_ex)\n",
    "        \n",
    "    return all_train_results, all_test_results, test_results_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resuts_all_runs_list = []\n",
    "resuts_external_list = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    all_train_results, all_test_results, test_results_ex = run_all(df_interaction_newID, all_interaction_matri_newLabel, embeddings_seq , n=10)\n",
    "    resuts_all_runs_list.append(all_test_results) \n",
    "    resuts_external_list.append(test_results_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_result = pd.DataFrame({'accuracy':[0], 'precision':[0], 'recall':[0],\n",
    "                           'f1_score':[0],'f2_score':[0], 'average_precision_score':[0],\n",
    "                           'ROC':[0]})\n",
    "for i in range(0,5):\n",
    "    all_result = all_result.append(pd.DataFrame(resuts_all_runs_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(resuts_external_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
